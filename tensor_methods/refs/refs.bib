@misc{grasedyckParameterdependentSmootherMultigrid2020,
  abstract = {The solution of parameter-dependent linear systems, by classical methods, leads to an arithmetic effort that grows exponentially in the number of parameters. This renders the multigrid method, which has a well understood convergence theory, infeasible. A parameter-dependent representation, e.g., a low-rank tensor format, can avoid this exponential dependence, but in these it is unknown how to calculate the inverse directly within the representation. The combination of these representations with the multigrid method requires a parameter-dependent version of the classical multigrid theory and a parameter-dependent representation of the linear system, the smoother, the prolongation and the restriction. A derived parameter-dependent version of the smoothing property, fulfilled by parameter-dependent versions of the Richardson and Jacobi methods, together with the approximation property prove the convergence of the multigrid method for arbitrary parameter-dependent representations. For a model problem low-rank tensor formats represent the parameter-dependent linear system, prolongation and restriction. The smoother, a damped Jacobi method, is directly approximated in the low-rank tensor format by using exponential sums. Proving the smoothing property for this approximation guarantees the convergence of the parameter-dependent method. Numerical experiments for the parameter-dependent model problem, with bounded parameter value range, indicate a grid size independent convergence rate.},
  author = {Grasedyck, Lars and Klever, Maren and L\"{o}bbert, Christian and Werthmann, Tim A.},
  url = {http://arxiv.org/abs/2008.00927},
  eprint = {2008.00927},
  eprinttype = {arxiv},
  keywords = {65N55; 15A69,Mathematics - Numerical Analysis},
  month = {08},
  title = {A Parameter-Dependent Smoother for the Multigrid Method},
  urldate = {2021-05-22},
  year = {2020},
}

@article{grasedyckDistributedHierarchicalSVD2018,
  abstract = {We consider tensors in the Hierarchical Tucker format and suppose the tensor data to be distributed among several compute nodes. We assume the compute nodes to be in a one-to-one correspondence with the nodes of the Hierarchical Tucker format such that connected nodes can communicate with each other. An appropriate tree structure in the Hierarchical Tucker format then allows for the parallelization of basic arithmetic operations between tensors with a parallel runtime that grows like , where d is the tensor dimension. We introduce parallel algorithms for several tensor operations, some of which can be applied to solve linear equations directly in the Hierarchical Tucker format using iterative methods such as conjugate gradients or multigrid. We present weak scaling studies, which provide evidence that the runtime of our algorithms indeed grows like . Furthermore, we present numerical experiments in which we apply our algorithms to solve a parameter-dependent diffusion equation in the Hierarchical Tucker format by means of a multigrid algorithm.},
  author = {Grasedyck, Lars and L\"{o}bbert, Christian},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2174},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2174},
  doi = {10.1002/nla.2174},
  issn = {1099-1506},
  journal = {Numerical Linear Algebra with Applications},
  keywords = {Hierarchical Tucker,HT,multigrid,parallel algorithms,SVD,tensor arithmetic},
  langid = {english},
  number = {6},
  pages = {e2174},
  title = {Distributed Hierarchical {{SVD}} in the {{Hierarchical Tucker}} Format},
  urldate = {2021-05-22},
  volume = {25},
  year = {2018},
}

@article{grasedyckHierarchicalSingularValue2010,
  abstract = {We define the hierarchical singular value decomposition (SVD) for tensors of order \$d\textbackslash geq2\$. This hierarchical SVD has properties like the matrix SVD (and collapses to the SVD in \$d=2\$), and we prove these. In particular, one can find low rank (almost) best approximations in a hierarchical format (\$\textbackslash mathcal\{H\}\$-Tucker) which requires only \$\textbackslash mathcal\{O\}((d-1)k\^3+dnk)\$ parameters, where d is the order of the tensor, n the size of the modes, and k the (hierarchical) rank. The \$\textbackslash mathcal\{H\}\$-Tucker format is a specialization of the Tucker format and it contains as a special case all (canonical) rank k tensors. Based on this new concept of a hierarchical SVD we present algorithms for hierarchical tensor calculations allowing for a rigorous error analysis. The complexity of the truncation (finding lower rank approximations to hierarchical rank k tensors) is in \$\textbackslash mathcal\{O\}((d-1)k\^4+dnk\^2)\$ and the attainable accuracy is just 2--3 digits less than machine precision.},
  author = {Grasedyck, Lars},
  publisher = {Society for Industrial and Applied Mathematics},
  url = {https://epubs.siam.org/doi/abs/10.1137/090764189},
  doi = {10.1137/090764189},
  issn = {0895-4798},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  month = {01},
  number = {4},
  pages = {2029--2054},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title = {Hierarchical {{Singular Value Decomposition}} of {{Tensors}}},
  urldate = {2021-05-22},
  volume = {31},
  year = {2010},
}

